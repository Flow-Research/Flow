# Flow Specifications & Documentation
# Rules for technical documentation and design documents

## Purpose
This directory contains architectural specifications, design documents, and technical changelogs for Flow.

## Document Types

### Architecture Specs (00-11_*.md)
- High-level system architecture
- Layer definitions and responsibilities
- Technology choices and rationale
- Integration patterns

### Changelogs (changelog/*.md)
- Detailed technical changelogs
- Implementation notes
- Migration guides
- Breaking changes

## Technical Documentation Structure

When the AI generates technical explanations, design documents, or implementation plans, use this **7-section structure**:

### 1. High-Level Design
(2-4 sentences)
- Goal of the work item
- Key components/structs/modules added or modified
- Runtime interaction overview

Example:
```
**Goal**: Implement persistent DHT storage for Kademlia using RocksDB to enable production-ready peer routing that survives node restarts.

**Key Components**: 
- RocksDbStore: Implements libp2p RecordStore trait
- StorageConfig: Configuration for RocksDB options
- Column families: records, providers, index

**Runtime**: NetworkManager initializes RocksDbStore on startup, passes to Kademlia behaviour, DHT records persist across restarts, queries read from disk.
```

### 2. Data Structures and Types
Define new structs, enums, type aliases with 1-2 sentence purpose:

```rust
pub struct RocksDbStore {
    db: Arc<DB>,
    peer_id: PeerId,
    config: StorageConfig,
}
// Purpose: Implements libp2p RecordStore backed by RocksDB for persistent DHT storage.

pub struct StorageConfig {
    pub db_path: PathBuf,
    pub max_records: usize,
    pub max_providers_per_key: usize,
    pub enable_compression: bool,
}
// Purpose: Configuration for RocksDB options and DHT storage limits.
```

### 3. Step-by-Step Implementation Plan
Numbered list of concrete steps:

```
1. Add rocksdb dependency to Cargo.toml with features [multi-threaded-cf, lz4, snappy]
2. Create storage/rocksdb_store.rs module
3. Define RocksDbStore struct and StorageConfig
4. Implement RecordStore trait methods (get, put, remove, records, provided)
5. Add column family setup for records, providers, index
6. Integrate into NetworkManager: replace MemoryStore with RocksDbStore
7. Update tests to use tempfile for RocksDB isolation
8. Add telemetry logging (info for lifecycle, debug for operations)
```

### 4. Code Changes by File
For each file:

```
FILE: path/to/file.rs
```rust
// Code changes here
// Include imports, definitions, functions
// Show wiring into existing code
```

Keep snippets conceptually compilable (no obviously missing types/imports).

### 5. Tests
Propose tests with enough coverage:

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    
    #[test]
    fn test_store_get_put() {
        let temp = TempDir::new().unwrap();
        let store = RocksDbStore::new(temp.path(), config).unwrap();
        // Test logic
    }
    
    #[test]
    fn test_persistence() {
        // Verify data survives store drop and reopen
    }
    
    #[tokio::test]
    async fn test_concurrent_access() {
        // Verify thread-safety
    }
}
```

Generate test code for:
- Unit tests (component behavior)
- Integration tests (interactions with other components)
- Edge cases (errors, boundary conditions)
- Concurrency (if applicable)

### 6. Usage Example
Short example of using the new code:

```rust
// Example: Using RocksDbStore with NetworkManager

let config = StorageConfig {
    db_path: data_dir.join("dht"),
    max_records: 10_000,
    enable_compression: true,
};

let store = RocksDbStore::new(&config.db_path, config)?;

// Pass to Kademlia
let kademlia = kad::Behaviour::with_config(
    peer_id,
    store,
    kad_config,
);
```

### 7. Notes and Trade-offs
List:
- Limitations
- Trade-offs made
- Performance characteristics
- Future extensibility

Example:
```
**Limitations:**
- RocksDB single-writer model (one process per DB)
- Column family overhead for small datasets

**Trade-offs:**
- serde_json over bincode for serialization: debuggability > slight performance gain
- Bloom filters (10 bits/key): 0.95% false positive rate, 1.25 bytes/record overhead

**Future Extensibility:**
- Easy to add new column families for additional data
- Compatible with future GossipSub persistence (separate CF)
- Config allows tuning compression, cache sizes
```

## Documentation Best Practices

### Clarity Over Completeness
- Be concise but precise
- Use code examples liberally
- Avoid jargon unless necessary
- Link to related specs when relevant

### Version Control
- Date all changelogs (YYYY-MM-DD format)
- Use semantic versioning for releases
- Mark deprecated features clearly
- Provide migration guides for breaking changes

### Code Snippets
- Use syntax highlighting (```rust)
- Show imports if non-obvious
- Include error handling in examples
- Comment complex logic

### Diagrams & Visuals
- ASCII diagrams for architecture
- Mermaid for sequence/flow diagrams
- Keep diagrams up-to-date with code

## Changelog Format

Follow "Keep a Changelog" style:

```markdown
# Changelog

## [0.2.0] - 2025-11-21

### Added
- libp2p networking with Kademlia DHT
- Persistent DHT storage using RocksDB
- NetworkManager abstraction with lifecycle management

### Changed
- Migrated Event Store from Sled to RocksDB
- Refactored AI module into 7 focused components

### Fixed
- Race condition in Event Store append operations

### Breaking Changes
- Storage backend change requires fresh initialization
- Import path changes: `ai_pipeline` â†’ `ai`
```

## Review Checklist

Before committing documentation:
- [ ] Technical reports follow 7-section structure
- [ ] Code examples are complete and correct
- [ ] Changelog entries are user-facing (not internal details)
- [ ] Breaking changes clearly documented with migration path
- [ ] Examples include error handling
- [ ] Trade-offs and limitations noted
- [ ] Future extensibility considerations included
- [ ] Links to related specs are correct
- [ ] Date and version information present
