# Flow Network Module - libp2p P2P Networking
# Specialized rules for networking code

## Module Purpose
This module implements Flow's P2P networking layer using libp2p:
- Peer discovery and routing (Kademlia DHT)
- Secure connections (Noise protocol + Yamux)
- Multi-transport (QUIC primary, TCP fallback)
- Peer lifecycle management and statistics
- Persistent state (DHT records, peer registry) via RocksDB

## Critical Design Principles

### 1. Modularity & Abstraction
The NetworkManager provides the primary interface. Keep libp2p details internal.

REQUIRED PATTERN:
```rust
// Public API (in manager.rs)
pub struct NetworkManager {
    // Internal state - not exposed
    node_data: NodeData,
    command_tx: mpsc::UnboundedSender<NetworkCommand>,
    // ...
}

impl NetworkManager {
    pub async fn new(node_data: &NodeData) -> Result<Self, AppError> { /* ... */ }
    pub async fn start(&self, config: &NetworkConfig) -> Result<(), AppError> { /* ... */ }
    pub async fn stop(&mut self) -> Result<(), AppError> { /* ... */ }
    pub async fn peer_count(&self) -> Result<usize, AppError> { /* ... */ }
    // Public methods only
}

// Internal event loop (private)
struct NetworkEventLoop {
    swarm: Swarm<FlowBehaviour>,
    command_rx: mpsc::UnboundedReceiver<NetworkCommand>,
    peer_registry: Arc<PersistentPeerRegistry>,
}
```

### 2. State & Lifecycle Management

#### State Rules
- All shared state must be thread-safe (Arc + RwLock/Mutex)
- Use message passing (channels) for cross-task communication
- Store NodeData, not derive it multiple times
- Peer registry state persists across restarts (RocksDB)

#### Lifecycle Rules
- NetworkManager has clear new() → start() → stop() lifecycle
- start() spawns NetworkEventLoop in background task
- Event loop runs until shutdown command received
- stop() sends shutdown, waits for cleanup, flushes persistent state
- IMPORTANT: NetworkManager cannot restart (one-shot channel limitation)
  - Document this clearly
  - If restart needed, create new NetworkManager instance

REQUIRED PATTERN:
```rust
impl NetworkManager {
    pub async fn start(&self, config: &NetworkConfig) -> Result<(), AppError> {
        // Validate not already started
        // Build Swarm with transports, behaviour
        // Take command_rx from Arc<Mutex<Option<>>>
        // Create PersistentPeerRegistry with RocksDB
        // Spawn NetworkEventLoop task
        // Store task handle for stop()
    }
    
    pub async fn stop(&mut self) -> Result<(), AppError> {
        // Send shutdown command
        // Await task completion
        // Flush peer registry to disk
        // Log completion
    }
}

struct NetworkEventLoop {
    async fn run(mut self) -> Result<(), AppError> {
        loop {
            tokio::select! {
                Some(cmd) = self.command_rx.recv() => {
                    if self.handle_command(cmd).await {
                        break; // Shutdown requested
                    }
                }
                event = self.swarm.select_next_some() => {
                    self.handle_swarm_event(event).await;
                }
            }
        }
        // Final cleanup: flush peer registry
        self.peer_registry.flush()?;
        Ok(())
    }
}
```

### 3. Testing Requirements

#### Unit Tests
- Component isolation: test manager, peer_registry, storage separately
- Use tempfile::TempDir for RocksDB test databases
- Mock external dependencies where possible

#### Integration Tests  
- Multi-node scenarios: spawn 2-3 NetworkManagers in same test
- Verify peer discovery (Kademlia)
- Verify connection establishment (QUIC/TCP)
- Verify peer registry persistence
- Test graceful shutdown and restart

REQUIRED PATTERN:
```rust
#[tokio::test]
async fn test_two_nodes_connect() {
    let (node1_manager, _temp1) = create_test_manager(4001).await;
    let (node2_manager, _temp2) = create_test_manager(4002).await;
    
    node1_manager.start(&config1).await.unwrap();
    node2_manager.start(&config2).await.unwrap();
    
    // Node2 dials Node1
    node2_manager.dial_peer(node1_addr).await.unwrap();
    
    // Wait for connection
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    assert_eq!(node1_manager.peer_count().await.unwrap(), 1);
    assert_eq!(node2_manager.peer_count().await.unwrap(), 1);
    
    node1_manager.stop().await.unwrap();
    node2_manager.stop().await.unwrap();
}
```

### 4. Future Compatibility

This module MUST remain compatible with:

#### GossipSub (Pub/Sub Messaging)
- FlowBehaviour will add GossipSub alongside Kademlia
- NetworkManager will expose publish() and subscribe() methods
- Event loop will handle GossipSub::Message events

Design for this:
```rust
// Leave room in FlowBehaviour
#[derive(NetworkBehaviour)]
pub struct FlowBehaviour {
    pub kademlia: kad::Behaviour<RocksDbStore>,
    // FUTURE: pub gossipsub: gossipsub::Behaviour,
}

// Leave room in NetworkCommand enum
pub enum NetworkCommand {
    Shutdown,
    GetPeerCount { response: oneshot::Sender<usize> },
    // FUTURE: Publish { topic: String, data: Vec<u8> },
    // FUTURE: Subscribe { topic: String },
}
```

#### mDNS (Local Peer Discovery)
- FlowBehaviour will add mDNS for LAN discovery
- NetworkEventLoop will handle mDNS::Event for discovered peers

#### REST/WebSocket Network State Exposure
- NetworkManager methods are already async and return Results
- Easy to wrap in Axum/WebSocket handlers in api/ module
- PeerRegistry stats already structured for serialization

### 5. Telemetry & Observability

REQUIRED: Use tracing at appropriate levels

```rust
// At component boundaries (manager, event loop)
#[instrument(skip(self, config), fields(peer_id = %self.local_peer_id()))]
pub async fn start(&self, config: &NetworkConfig) -> Result<(), AppError> {
    info!("Starting network manager");
    // ...
    info!("Network manager started successfully");
}

// In event loop
async fn handle_swarm_event(&mut self, event: SwarmEvent) {
    match event {
        SwarmEvent::ConnectionEstablished { peer_id, .. } => {
            info!(peer = %peer_id, "Connection established");
            // ...
        }
        SwarmEvent::ConnectionClosed { peer_id, cause, .. } => {
            warn!(peer = %peer_id, cause = ?cause, "Connection closed");
            // ...
        }
        // ...
    }
}

// Low-level details at debug level
debug!(key = %key, "DHT record stored");
```

### 6. File-Specific Guidance

#### manager.rs
- Primary public interface to networking
- Manages lifecycle and command dispatch
- No direct libp2p types in public API
- All operations return Result<T, AppError>

#### behaviour.rs  
- Defines FlowBehaviour (NetworkBehaviour derive)
- Combines libp2p protocols (Kademlia, future GossipSub/mDNS)
- Keep thin - just protocol composition

#### peer_registry.rs
- In-memory peer tracking (connections, stats, addresses)
- Thread-safe (RwLock)
- No persistence logic here (that's in persistent_peer_registry.rs)

#### persistent_peer_registry.rs
- Wraps PeerRegistry with RocksDB persistence
- Background flush task (periodic + on shutdown)
- Loads state on initialization

#### storage/rocksdb_store.rs
- Implements kad::store::RecordStore for persistent DHT
- Column families for records, providers, index
- Serialization via serde_json (not bincode - debuggability matters)

#### storage/peer_registry_store.rs
- RocksDB wrapper for peer registry data
- Column families for active_peers, known_peers, peer_stats
- Handles Instant ↔ SystemTime conversions for persistence

#### config.rs
- NetworkConfig: enable_quic, listen_port, bootstrap_peers
- from_env() with sensible defaults
- No complex logic, just configuration

#### keypair.rs
- Converts Flow's Ed25519 keys to libp2p format
- Pure function, no state
- Well-tested for determinism

### 7. Common Pitfalls in Network Code

❌ DON'T:
- Expose libp2p types in public NetworkManager API
- Spawn background tasks without tracking handles
- Use blocking I/O in async event loop
- Open RocksDB from multiple processes/threads
- Forget to flush persistent state on shutdown
- Use println! instead of tracing
- Implement restart in NetworkManager (channel limitation)

✅ DO:
- Use message passing (channels) for task communication
- Wrap libp2p details behind clean interfaces
- Use tempfile in tests for DB isolation
- Log at appropriate levels (info for lifecycle, debug for details)
- Test multi-node scenarios in integration tests
- Document one-shot limitation clearly
- Flush all RocksDB stores on shutdown

### 8. Code Review Checklist

Before submitting network code, verify:
- [ ] Public API uses Flow types, not libp2p types
- [ ] Lifecycle is clear (new/start/stop, no leaked tasks)
- [ ] Shared state is thread-safe (Arc + RwLock/Mutex)
- [ ] Background tasks have shutdown mechanism
- [ ] RocksDB stores flush on shutdown
- [ ] Tests use tempfile for isolation
- [ ] Logging uses tracing with structured fields
- [ ] Compatible with GossipSub/mDNS addition (leave room)
- [ ] Error handling returns AppError with context
- [ ] Integration tests cover multi-node scenarios
